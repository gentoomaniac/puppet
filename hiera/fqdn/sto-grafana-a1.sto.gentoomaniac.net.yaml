---
classes:
  - docker::networks
  - docker::run_instance

ressources:
  zfs:
    "datapool/grafana":
      ensure: present
    "datapool/graphite-exporter":
      ensure: present
    "datapool/otelcol":
      ensure: present
    "datapool/prometheus":
      ensure: present
    "datapool/tempo":
      ensure: present
    "datapool/traefik":
      ensure: present
    "datapool/shelly-exporter":
      ensure: present
  file:
    "/srv/traefik/conf":
      ensure: directory
      require: Zfs[datapool/traefik]
    "/srv/traefik/ssl":
      ensure: directory
      require: Zfs[datapool/traefik]
    "/srv/traefik/ssl/key.pem":
      ensure: present
      content: "%{lookup('secret_srv_gentoomaniac_net_key')}"
      require: File[/srv/traefik/ssl]
      notify: Docker::Run[traefik]
    "/srv/traefik/ssl/cert.pem":
      ensure: present
      content: "%{lookup('secret_srv_gentoomaniac_net_cert')}"
      require: File[/srv/traefik/ssl]
      notify: Docker::Run[traefik]
    "/srv/traefik/conf/certs.toml":
      ensure: present
      content: |
        [tls.stores]
          [tls.stores.default]
          [tls.stores.default.defaultCertificate]
            certFile = "/ssl/cert.pem"
            keyFile = "/ssl/key.pem"
      require:
        - File[/srv/traefik/ssl/cert.pem]
        - File[/srv/traefik/ssl/key.pem]
    "/srv/tempo/data":
      ensure: directory
      owner: 10001
      group: 10001
      require: Zfs[datapool/tempo]
    "/srv/tempo/tls":
      ensure: directory
      require: Zfs[datapool/tempo]
    "/srv/tempo/tls/key.pem":
      ensure: present
      content: "%{lookup('secret_srv_gentoomaniac_net_key')}"
      require: File[/srv/tempo/tls]
      notify: Docker::Run[tempo]
    "/srv/tempo/tls/cert.pem":
      ensure: present
      content: "%{lookup('secret_srv_gentoomaniac_net_cert')}"
      require: File[/srv/tempo/tls]
      notify: Docker::Run[tempo]
    "/srv/tempo/tempo.yaml":
      ensure: present
      content: |
        stream_over_http_enabled: true
        server:
          http_listen_port: 3200
          log_level: warn

        query_frontend:
          search:
            duration_slo: 5s
            throughput_bytes_slo: 1.073741824e+09
          trace_by_id:
            duration_slo: 5s

        distributor:
          receivers:                           # this configuration will listen on all ports and protocols that tempo is capable of.
            otlp:
              protocols:
                http:
                grpc:

        ingester:
          max_block_duration: 5m               # cut the headblock when this much time passes. this is being set for demo purposes and should probably be left alone normally

        compactor:
          compaction:
            block_retention: 1h                # overall Tempo trace retention. set for demo purposes

        metrics_generator:
          registry:
            external_labels:
              source: tempo
              cluster: docker-compose
          storage:
            path: /var/tempo/generator/wal
            remote_write:
              - url: http://prometheus.srv.gentoomaniac.net:9090/api/v1/write
                send_exemplars: true
          traces_storage:
            path: /var/tempo/generator/traces

        storage:
          trace:
            backend: local                     # backend configuration to use
            wal:
              path: /var/tempo/wal             # where to store the wal locally
            local:
              path: /var/tempo/blocks

        overrides:
          defaults:
            metrics_generator:
              processors: [service-graphs, span-metrics, local-blocks] # enables metrics generator
              generate_native_histograms: both
      require: Zfs[datapool/tempo]
    "/srv/prometheus/data":
      ensure: directory
      owner: nobody
      group: nogroup
      require: Zfs[datapool/prometheus]
    "/srv/prometheus/prometheus.yml":
      ensure: present
      content: |
        global:
          scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
          evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
          # scrape_timeout is set to the global default (10s).

        # Alertmanager configuration
        alerting:
          alertmanagers:
            - static_configs:
                - targets:
                  # - alertmanager:9093

        # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
        rule_files:

        scrape_configs:
          - job_name: "prometheus"
            static_configs:
              - targets:
                - localhost:9090
          - job_name: "nodeexporter"
            static_configs:
              - targets:
                - sto-coredns-a1.sto.gentoomaniac.net:9100
                - sto-coredns-c1.sto.gentoomaniac.net:9100
                - sto-grafana-a1.sto.gentoomaniac.net:9100
                - sto-infra-a1.sto.gentoomaniac.net:9100
                - sto-minecraft-b1.sto.gentoomaniac.net:9100
                - sto-nzbget-a1.sto.gentoomaniac.net:9100
                - sto-plex-a1.sto.gentoomaniac.net:9100
                - sto-proxmox-a1.sto.gentoomaniac.net:9100
                - sto-vault-a1.sto.gentoomaniac.net:9100
          - job_name: "coredns"
            static_configs:
              - targets:
                - sto-coredns-a1.sto.gentoomaniac.net:9153
                - sto-coredns-c1.sto.gentoomaniac.net:9153
          - job_name: "freenas"
            static_configs:
              - targets:
                - sto-grafana-a1.sto.gentoomaniac.net:9109
          - job_name: "iot"
            scheme: https
            static_configs:
              - targets:
                - shelly-exporter.srv.gentoomaniac.net:443
      require: Zfs[datapool/prometheus]
    "/srv/graphite-exporter/mappings.conf":
      ensure: present
      content: |
        mappings:

        ############################################
        # cpu
        #
        # servers.HOST.cpu.1.percent.nice
        # servers.HOST.cpu.0.cpu.idle
        #
        # Note: 'gauge' or 'cpu' part depends
        # on 'Report CPU usage in percent' setting
        ############################################

        - match: "servers.*.cpu.*.*.*"
          name: "cpu"
          labels:
            job: "truenas"
            instance: "${1}"
            cpu_id: "${2}"
            stat_type: ${3}
            state: ${4}

        ######################################
        # cputemp
        #
        # servers.HOST.cputemp.2.temperature
        ######################################

        - match: "servers.*.cputemp.*.*"
          name: "cputemp_${3}"
          labels:
            job: "truenas"
            instance: "${1}"
            cpu_id: "${2}"

        ###################################################
        # CPU Aggregation
        #
        # servers.HOST.aggregation.cpu-sum.gauge.idle
        # servers.HOST.aggregation.cpu-average.gauge.nice
        # servers.HOST.aggregation.cpu-average.cpu.idle
        #
        # Note: 'gauge' or 'cpu' part depends
        # on 'Report CPU usage in percent' setting
        ###################################################

        - match: servers.*.aggregation.*.gauge.*
          name: aggregation_${2}
          labels:
            job: "truenas"
            instance: "${1}"
            state: ${3}
            stat_type: gauge

        - match: servers.*.aggregation.*.cpu.*
          name: aggregation_${2}
          labels:
            job: "truenas"
            instance: "${1}"
            state: ${3}
            stat_type: cpu


        #####################################################
        # Disk
        # servers.HOST.disk.sda2.pending_operations
        # servers.HOST.disk.sdc1.disk_ops.read
        ######################################################
        - match: "servers.*.disk.*.*"
          name: "disk_${3}"
          labels:
            instance: "${1}"
            job: "truenas"
            type: "disk"
            disk: "${2}"

        - match: "servers.*.disk.*.*.*"
          name: "disk_${3}_${4}"
          labels:
            instance: "${1}"
            job: "truenas"
            type: "disk"
            disk: "${2}"

        - match: "servers.*.disktemp.*.*"
          name: "disktemp_${3}"
          labels:
            instance: "${1}"
            job: "truenas"
            type: "disktemp"
            disk: "${2}"

        ############################################################
        # df
        # servers.HOST.df.var-lib-systemd-coredump.df_complex.free
        ############################################################
        - match: "servers.*.df.*.*.*"
          name: "${3}_${4}"
          labels:
            instance: "${1}"
            job: truenas
            type: df
            filesystem: "${2}"

        ########################################
        # Uptime
        #
        # servers.HOST.uptime.uptime
        ########################################

        - match: "servers.*.uptime.uptime"
          name: "uptime"
          labels:
            instance: "${1}"
            job: "truenas"

        ########################################
        # Memory
        #
        # servers.HOST.memory.memory.used
        ########################################
        - match: "servers.*.memory.*.*"
          name: "${2}_${3}"
          labels:
            instance: "${1}"
            job: "truenas"

        ########################################
        # Swap
        #
        # servers.HOST.swap.swap.used
        ########################################
        - match: "servers.*.swap.*.*"
          name: "${2}_${3}"
          labels:
            instance: "${1}"
            job: "truenas"

        ####################################################
        # processes_state
        #
        # servers.HOST.processes.ps_state.blocked
        ####################################################
        - match: "servers.*.processes.ps_state.*"
          name: "processes_state"
          labels:
            instance: "${1}"
            job: "truenas"
            state: "${2}"

        ####################################################
        # processes_xxx
        #
        # servers.HOST.processes.fork_rate
        ####################################################
        - match: "servers.*.processes.*"
          name: "processes_${2}"
          labels:
            instance: "${1}"
            job: "truenas"

        #######################################
        # zfs_arc
        ######################################
        - match: "servers.*.zfs_arc.*"
          name: "zfs_arc_${2}"
          labels:
            instance: "${1}"
            job: "truenas"

        - match: "servers.*.zfs_arc.*.*"
          name: "zfs_arc_${2}_${3}"
          labels:
            instance: "${1}"
            job: "truenas"

        - match: "servers.*.zfs_arc.*.*.*"
          name: "zfs_arc_${2}_${3}_${4}"
          labels:
            instance: "${1}"
            job: "truenas"

        #######################################
        # rrdcached
        ######################################
        - match: "servers.*.rrdcached.*"
          name: "rrdcached_${2}"
          labels:
            instance: "${1}"
            job: "truenas"

        - match: "servers.*.rrdcached.*.*"
          name: "rrdcached_${2}_${3}"
          labels:
            instance: "${1}"
            job: "truenas"

        - match: "servers.*.rrdcached.*.*.*"
          name: "rrdcached_${2}_${3}_${4}"
          labels:
            instance: "${1}"
            job: "truenas"


        ##############################################
        # nfsstat_nfsv4_ops
        # servers.HOST.nfsstat.nfsv4_ops.nfsstat.nfsv4_op_write
        # servers.HOST.nfsstat.nfsv3_ops.nfsstat.nfsv3_op_lookup
        ##############################################
        - match: servers.*.nfsstat.*.nfsstat.*
          name: nfsstat_${2}
          labels:
            instance: "${1}"
            job: "truenas"
            op: "$3"

        ############################################################
        # nfsstat_server
        # servers.HOST.nfsstat.server.nfsstat.fh_stale
        ###########################################################
        - match: servers.*.nfsstat.server.nfsstat.*
          name: nfsstat_server
          labels:
            instance: "${1}"
            job: "truenas"
            stat: "${2}"

        ##############################################
        # Load
        # servers.HOST.load.load.shortterm
        # servers.HOST.load.load.midterm
        # servers.HOST.load.load.longterm
        #############################################
        - match: "servers.*.load.load.*"
          name: "load_average"
          labels:
            instance: "${1}"
            job: "truenas"
            period: "${2}"

        ################################################################
        # interface
        # servers.HOST.interface.kube-dummy-if.if_dropped.rx
        ################################################################
        - match: servers.*.interface.*.*.*
          name: interface_${3}
          labels:
            instance: ${1}
            job: truenas
            iface: ${2}
            direction: ${4}


        ####################
        # Generic fallbacks
        ####################

        - match: "servers.*.*.*"
          name: "${2}_${3}"
          labels:
            instance: "${1}"
            job: "truenas"
            fallback_match: "1"
            type: "${2}"

        - match: "servers.*.*.*.*"
          name: "${2}_${3}_${4}"
          labels:
            instance: "${1}"
            job: "truenas"
            fallback_match: "2"
            type: "${2}"

        - match: "servers.*.*.*.*.*"
          name: "${2}_${3}_${4}_${5}"
          labels:
            instance: "${1}"
            job: "truenas"
            fallback_match: "3"
            type: "${2}"

        - match: "servers.*.*.*.*.*.*"
          name: "${2}_${3}_${4}_${5}_${6}"
          labels:
            instance: "${1}"
            job: "truenas"
            type: "${2}"
            fallback_match: "4"
      require: Zfs[datapool/graphite-exporter]
    "/srv/otelcol/config.yaml":
      ensure: present
      content: |
        receivers:
          otlp:
            protocols:
              http:
                endpoint: 0.0.0.0:4318

        exporters:
          otlphttp:
            endpoint: http://tempo:4318
          debug:

        service:
          telemetry:
            logs:
              level: "DEBUG"
              development: true
              encoding: "json"
          pipelines:
            traces:
              receivers: [otlp]
              exporters: [otlphttp]
      require: Zfs[datapool/otelcol]
    "/srv/shelly-exporter/config.yaml":
      ensure: present
      content: |
        global:
          user: "marco"
          password: "%{lookup('secret_shelly_password')}"

        devices:
        - type: "SHPLG-S"
          ip: "10.1.3.100"
        - type: "SHPLG-S"
          ip: "10.1.3.104"
        - type: "SHPLG-S"
          ip: "10.1.3.112"
        - type: "SHPLG-S"
          ip: "10.1.3.117"
        - type: "SHPLG-S"
          ip: "10.1.3.128"
        - type: "SHPLG-S"
          ip: "10.1.3.130"
        - type: "SHPLG-S"
          ip: "10.1.3.149"
        - type: "SHPLG-S"
          ip: "10.1.3.150"
        - type: "SHPLG-S"
          ip: "10.1.3.154"
        - type: "SHPLG-S"
          ip: "10.1.3.156"
        - type: "SHPLG-S"
          ip: "10.1.3.171"
        - type: "SHPLG-S"
          ip: "10.1.3.173"
        - type: "SHPLG-S"
          ip: "10.1.3.186"

      require: Zfs[datapool/shelly-exporter]
      notify: Docker::Run[shelly-exporter]

docker::networks::networks:
  web:
    ensure: "present"
  backend:
    ensure: "present"

docker::run_instance::instance:
  traefik:
    image: traefik:v3.1
    command: >
      --api.insecure=true
      --api.dashboard=true
      --entrypoints.tempo.address=:3200
      --entrypoints.otel.address=:4318
      --entrypoints.ssl.address=:443
      --entrypoints.prometheus.address=:9090
      --serverstransport.insecureskipverify=true
      --providers.docker
      --providers.docker.exposedbydefault=false
      --providers.file.directory=/conf
      --providers.file.watch=true
    dns:
      - 10.1.1.52
      - 10.1.1.53
    env:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Berlin
    extra_parameters:
      - --restart=unless-stopped
    net:
      - web
    ports:
      - 443:443/tcp
      - 3200:3200/tcp
      - 4318:4318/tcp
      - 8080:8080/tcp
    pull_on_start: true
    require:
      - Class[docker]
      - Docker_network[web]
      - File[/srv/traefik/conf/certs.toml]
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /srv/traefik/ssl:/ssl:ro
      - /srv/traefik/conf:/conf:ro

  tempo:
    image: grafana/tempo:latest
    dns:
      - 10.1.1.52
      - 10.1.1.53
    labels:
      - traefik.enable=true
      - traefik.http.routers.tempo.rule=Host(`tempo.srv.gentoomaniac.net`)
      - traefik.http.services.tempo.loadbalancer.server.port=3200
      - traefik.http.routers.tempo.entrypoints=tempo
      - traefik.http.routers.tempo.tls=true
      - traefik.http.middlewares.tempo-whitelist.ipallowlist.sourcerange=10.1.1.0/24,192.168.0.0/16,172.0.0.0/8
      - traefik.http.routers.tempo.middlewares=tempo-whitelist
      - traefik.http.routers.tempo.service=tempo@docker
      - traefik.docker.network=web
    command: >
      --config.file=/etc/tempo.yaml
    extra_parameters:
      - --restart=unless-stopped
    volumes:
      - /srv/tempo/tempo.yaml:/etc/tempo.yaml:ro
      - /srv/tempo/data:/var/tempo
    net:
      - web
      - backend
    expose:
      - 3200
      - 4318
    require:
      - Class[docker]
      - Zfs[datapool/tempo]

  prometheus:
    image: prom/prometheus:latest
    dns:
      - 10.1.1.52
      - 10.1.1.53
    labels:
      - traefik.enable=false
    command: >
      --web.enable-remote-write-receiver
      --config.file=/etc/prometheus/prometheus.yml
      --enable-feature=native-histograms
    extra_parameters:
      - --restart=unless-stopped
    volumes:
      - /srv/prometheus/data:/prometheus
      - /srv/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    net:
      - web
    ports:
      - 9090:9090/tcp
    require:
      - Class[docker]
      - Zfs[datapool/prometheus]
      - File[/srv/prometheus/data]
      - File[/srv/prometheus/prometheus.yml]

  grafana:
    image: grafana/grafana:latest
    dns:
      - 10.1.1.52
      - 10.1.1.53
    env:
      - GF_SERVER_ROOT_URL=https://grafana.srv.gentoomaniac.net
      # - GF_SECURITY_ADMIN_PASSWORD=
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel,grafana-piechart-panel,vonage-status-panel,doitintl-bigquery-datasource
      - GF_RENDERING_SERVER_URL=http://renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
      - GF_LOG_FILTERS=rendering:debug
    expose:
      - 3000/tcp
    extra_parameters:
      - --restart=unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.routers.grafana.rule=Host(`grafana.srv.gentoomaniac.net`)
      - traefik.http.routers.grafana.entrypoints=ssl
      - traefik.http.routers.grafana.tls=true
      - traefik.http.middlewares.grafana-whitelist.ipallowlist.sourcerange=10.1.1.0/24,192.168.0.0/16
      - traefik.http.routers.grafana.middlewares=grafana-whitelist
    net:
      - web
    pull_on_start: true
    username: 1000:1000
    volumes:
      - /srv/grafana:/var/lib/grafana
    require:
      - Class[docker]
      - Zfs[datapool/grafana]

  renderer:
    image: grafana/grafana-image-renderer:2.0.0
    env:
      - ENABLE_METRICS=true
    expose:
      - 8081/tcp
    extra_parameters:
      - --restart=unless-stopped
    net:
      - web
    pull_on_start: true
    require:
      - Class[docker]

  graphite-exporter:
    image: prom/graphite-exporter
    command: >
      --graphite.mapping-config=/tmp/mappings.conf
    # --log.level=debug
    ports:
      - 9108:9108/tcp
      - 2003:9109/tcp
      - 2003:9109/udp
    extra_parameters:
      - --restart=unless-stopped
    pull_on_start: true
    volumes:
      - /srv/graphite-exporter/mappings.conf:/tmp/mappings.conf:ro
    require:
      - Class[docker]
      - File[/srv/graphite-exporter/mappings.conf]


  otelcol:
    image: otel/opentelemetry-collector:0.109.0
    expose:
      - 4318
    extra_parameters:
      - --restart=unless-stopped
    net:
      - web
      - backend
    labels:
      - traefik.enable=true
      - traefik.http.routers.otelcol.rule=Host(`otelcol.srv.gentoomaniac.net`)
      - traefik.http.services.otelcol.loadbalancer.server.port=4318
      - traefik.http.routers.otelcol.entrypoints=otel
      - traefik.http.routers.otelcol.tls=true
      - traefik.http.middlewares.otelcol-whitelist.ipallowlist.sourcerange=10.1.1.0/24,192.168.0.0/16,172.0.0.0/8
      - traefik.http.routers.otelcol.middlewares=otelcol-whitelist
      - traefik.http.routers.otelcol.service=otelcol@docker
      - traefik.docker.network=web
    pull_on_start: true
    volumes:
      - /srv/otelcol/config.yaml:/etc/otelcol/config.yaml:ro
    require:
      - Class[docker]
      - File[/srv/otelcol/config.yaml]

  shelly-exporter:
    image: ghcr.io/gentoomaniac/shelly-exporter:v0.0.2-amd64
    expose:
      - 8080/tcp
    command: >
      -vv
      --config-file /etc/shelly-exporter.yaml
    extra_parameters:
      - --restart=unless-stopped
    net:
      - web
    labels:
      - traefik.enable=true
      - traefik.http.routers.shelly-exporter.rule=Host(`shelly-exporter.srv.gentoomaniac.net`)
      - traefik.http.routers.shelly-exporter.entrypoints=ssl
      - traefik.http.routers.shelly-exporter.tls=true
      - traefik.http.middlewares.shelly-exporter-whitelist.ipallowlist.sourcerange=10.1.1.0/24,192.168.0.0/16,172.0.0.0/8
      - traefik.http.routers.shelly-exporter.middlewares=shelly-exporter-whitelist
      - traefik.docker.network=web
    pull_on_start: true
    volumes:
      - /srv/shelly-exporter/config.yaml:/etc/shelly-exporter.yaml:ro
    require:
      - Class[docker]
      - File[/srv/shelly-exporter/config.yaml]
